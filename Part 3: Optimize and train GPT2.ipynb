{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize and train GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing a timing baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/gpu-usage-init.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each iteration would take about 720ms with batch size of 10 on a 3090 (vs 1000 on A100 with double the batch size!), so I expect my single-GPU training to be a bit slower than Andrej's.  With Andrej's A100, it's possible to process one epoch in 20 batches of size 16 with about 1000ms/batch, whereas with a 3090, it's possible to process one epoch in 33 batches of size 10 with 730ms/batch, so about 20% slower than Andrej's time (33x730/20x1000).\n",
    "- Also, Andrej suggests using nice numbers that have lots of multiples of 2.  I tried a batch size of 8, but in this specific use case, I found that using batch size of 10 vs 8 is slightly faster per epoch on a 3090. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Also, to properly time the code, it's important to wait until the GPU processes a batch before the CPU times the work.  It's possible that the CPU has loaded a batch onto GPU, GPU is still processing it, and the CPU has moved on to record the end time for the operation. To prevent this from happening, use torch.cuda.synchronize():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoaderLite(B=10,T=1024)\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "# create an optimizer object\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i in range(50):\n",
    "    t0 = time.time()\n",
    "    x,y = train_loader.next_batch()\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x,y)\n",
    "    #import code; code.interact(local=locals())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize() # wait for GPU to finish all the scheduled work\n",
    "    t1= time.time()\n",
    "    dt = (t1-t0)*1000 # time diff in seconds\n",
    "    tokens_per_sec = (train_loader.B*train_loader.T)/(t1-t0)\n",
    "    print(f'iteration {i}, loss = {loss.item()}, dt: {dt: .2f}ms, toks/sec: {tokens_per_sec:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We may change the batch size, so to get a more objective measure of training speed, look at tokens/second, which is 16.3K on an A100 and 14.1K on a 3090."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed precision logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First observe that we can check the type of the logits in our model by starting an interactive debugging session as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoaderLite(B=4,T=32)\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "# create an optimizer object\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i in range(50):\n",
    "    x,y = train_loader.next_batch()\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x,y)\n",
    "    import code; code.interact(local=locals())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'iteration {i}, loss = {loss.item()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(InteractiveConsole)\n",
    ">>> logits.dtype\n",
    "torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch supports up to FP64 precision, which is useful for scientific computing applications, but deep learning training can tolerate significantly lower precision than default FP32, however. \n",
    "- From A100 spec sheet, for example, it's possible to get 16x performance improvement by going down from FP32 to FP16 Tensor Core.\n",
    "- For deep learning, sparsity feature is not currently used, so disregard the second number in a cell when it's present.\n",
    "- We do not use INT8 for training since it implies a uniform distribution and we need a normal distribution provided by the float data types. INT8 is used for inference, however.\n",
    "- The memory bandwidth of an A100 is 1935GB/s, and most well-tuned application are bound more by memory than by speed.  Lowering precision means tensors will take less space in memory, making it easier to satisfy the memory bandwidth constraint.  \n",
    "- To summarize, by lowering precision, \"we can store more and access it faster\" (Andrej)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Source: https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/](./assets/a100-specs-short.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Source: https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/](./assets/tensor-core.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Tensor Core is an instruction in a given architecture, in the case above for a 4x4 multiply and add operations.  Any time we have multiply (and less importantly add) operations, which compose the majority of our GPT-2 transformer, these operations will be performed using Tensor Cores.  For example, the classifier head matrix multiply going from 768 to 50257 dominates the computations in GPT-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Source: https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf](./assets/tf32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With FP32 (torch.float32 default of PyTorch), both the input operands and the intermediate add/multiplies that compose individual elements of the result matrix are done in FP32. \n",
    "- We could switch to TF32, however, which uses the full 32 bits of FP32 for accumulator but just 19 bits for input operands due to lower number of mantissa bits as seen below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Source: https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf](./assets/exponent-mantissa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With TF32, we get an 8x speedup without needing to modify the code.  The outputs will still be in FP32 as seen below.  If our application can tolerate a little bit of imprecision, TF32 is a great option.  In practice,the difference between FP32 and TF32 is almost imperceptible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Source: https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf](./assets/tf32-mechanics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On my 24GB 3090, I was able to use batch size of 8 (or 10 max) with sequence length of 1024 (Andrej got away with batch size of 16 on 40GB A100):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the speedup with TF32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To enable TF32 in PyTorch, change the float32 matmul precision from it's default 'highest' to 'high' with torch.set_float32_matmul_precision('high'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoaderLite(B=10,T=1024)\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "...\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i in range(50):\n",
    "   ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On an A100, throughput increases from 16.1K tokens/second to about 49K tokens/second, so *about a 3X speedup*. \n",
    "- On 3090, this leads to *only about 40% speedup* from 14.1K tokens/second baseline to 19.6K tokens/second.  \n",
    "- While TF32 in principle offers an 8X speedup on an A100 (I couldn't find reliable official estimates for 3090 TF32 *tensor*), a lot of these workloads are memory bound.  Thus although a matrix multiplication could potentially happen 8X faster with TF32 compared to FP32, the output numbers are still FP32, and these get moved around through the memory system at a speed that's much slower than the GPU's ability to perform the calculations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bfloat16 to reduce memory bandwidth constraint "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's review the following once again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Source: https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf](./assets/exponent-mantissa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To address the memory bandwidth constraint, we can use Bfloat16 (BF16) to more aggressively crop the mantissa without changing the sign and exponent (range) of the number.  \n",
    "- Originally, FP16 was used, but this number format has a reduced range, causing issues that were patched by gradient scalers and similar solutions that introduced additional state and complexity. BF16 addresses these problems by preserving the original range of number.\n",
    "- Andrej recommends studying the torch.autocast portion of [mixed precision documentation](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html), which has a context manager *torch.autocast*.  This context manager is recommended around forward pass and loss calculation of the model only.\n",
    "- We thus only need to add one line of code as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    ...\n",
    "    optimizer.zero_grad()\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x,y)\n",
    "    loss.backward()\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using an interactive console breakpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    ...\n",
    "    optimizer.zero_grad()\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x,y)\n",
    "        import code; code.interact(local=locals())\n",
    "\n",
    "    loss.backward()\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We see that logits.dtype is indeed torch.bfloat16.  Our weights remain in FP32, however, as model.transformer.wte.weight.dtype is torch.float32.  This implies a mixed precision: PyTorch is keeping certain weights in full precision while converting others to bfloat16.  What gets converted at what point is not exactly clear, but the general guidelines are below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Source: https://pytorch.org/docs/stable/amp.html](./assets/tf32-vs-bfloat16-what-converts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thus matrix multiplications, addition, etc. get converted while layer norms, softmax, etc. do not since they are less robust to precision changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On an A100, our previous benchmark is 50K tokens/second and it goes up to 55K tokens/second with bfloat16, about a 10% speedup.\n",
    "- On a 3090, our previous benchmark is 19.6K tokens/second and it goes up to 27.5K tokens/second with bfloat16, about a 40% speedup, suggesting that 3090 was perhaps more memory-bound than the A100.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4030612244897958"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "27.5/19.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mintonano",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
